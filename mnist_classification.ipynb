{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1932396e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3474981",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8900a6df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tfds.core.DatasetInfo(\n",
       "    name='mnist',\n",
       "    full_name='mnist/3.0.1',\n",
       "    description=\"\"\"\n",
       "    The MNIST database of handwritten digits.\n",
       "    \"\"\",\n",
       "    homepage='http://yann.lecun.com/exdb/mnist/',\n",
       "    data_path='C:\\\\Users\\\\manik\\\\tensorflow_datasets\\\\mnist\\\\3.0.1',\n",
       "    file_format=tfrecord,\n",
       "    download_size=11.06 MiB,\n",
       "    dataset_size=21.00 MiB,\n",
       "    features=FeaturesDict({\n",
       "        'image': Image(shape=(28, 28, 1), dtype=uint8),\n",
       "        'label': ClassLabel(shape=(), dtype=int64, num_classes=10),\n",
       "    }),\n",
       "    supervised_keys=('image', 'label'),\n",
       "    disable_shuffling=False,\n",
       "    splits={\n",
       "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
       "        'train': <SplitInfo num_examples=60000, num_shards=1>,\n",
       "    },\n",
       "    citation=\"\"\"@article{lecun2010mnist,\n",
       "      title={MNIST handwritten digit database},\n",
       "      author={LeCun, Yann and Cortes, Corinna and Burges, CJ},\n",
       "      journal={ATT Labs [Online]. Available: http://yann.lecun.com/exdb/mnist},\n",
       "      volume={2},\n",
       "      year={2010}\n",
       "    }\"\"\",\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist_ds,mnist_info = tfds.load(name='mnist',with_info=True,as_supervised=True)\n",
    "mnist_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638c478",
   "metadata": {},
   "source": [
    "Note that mnist data consists of 60000 samples of train and 10000 samples of test but no validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "74c72d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create validatation data from train data(usually 10%)\n",
    "mnist_train,mnist_test = mnist_ds['train'],mnist_ds['test']\n",
    "no_valid_samples = 0.1*mnist_info.splits['train'].num_examples\n",
    "#typecase to int64\n",
    "no_valid_samples = tf.cast(no_valid_samples,tf.int64)\n",
    "\n",
    "no_test_samples = mnist_info.splits['test'].num_examples\n",
    "no_test_samples = tf.cast(no_test_samples,tf.int64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a196e903",
   "metadata": {},
   "source": [
    "## Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70585501",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(image,label):\n",
    "    image = tf.cast(image,tf.float32)\n",
    "    image /= 255. #dot is to make sure result to be in float\n",
    "    return image, label\n",
    "\n",
    "scale_train_valid_data = mnist_train.map(scale)\n",
    "scale_test = mnist_test.map(scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe596851",
   "metadata": {},
   "source": [
    "It's possible that the targets are stored in ascending order, resulting in the first X batches having only zero targets and the other batches having only one as a target.Since we'll be batching, it would be better shuffle the data. It should be as randomly spread as possible so that batching works as intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "daa0ad18",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 10000 #10000 samples at a time. If BUFFER_SIZE=1, no shuffling. If BUFFER_SIZE>no.of samples, shuffling shuffling \n",
    "                        #takesplace uniformly once.\n",
    "\n",
    "shuffled_train_valid = scale_train_valid_data.shuffle(BUFFER_SIZE)\n",
    "validation_data = shuffled_train_valid.take(no_valid_samples)\n",
    "train_data = shuffled_train_valid.skip(no_valid_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45eafb",
   "metadata": {},
   "source": [
    "## mini batch gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "438fdadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100 #hyper parameter need to be tuned\n",
    "train_data = train_data.batch(BATCH_SIZE) #No need to batch for validation data since we do not back propagate validation data\n",
    "validation_data = validation_data.batch(no_valid_samples) #since model expects validation and test data also be in batches.\n",
    "test_data = scale_test.batch(no_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data)) #next takes next batch for validation. Since there is only \n",
    "                                                                    #one batch, load only once\n",
    "#test_inputs, test_targets = next(iter(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604df0c2",
   "metadata": {},
   "source": [
    "## Model\n",
    "* Input layers has 28 x 28 = 784 neurons, \n",
    "* 2 hidden layers each with 50 neurons, and\n",
    "* one output layer with 10 neurons each for each digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b466bf0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tune these input parameters for better results\n",
    "input_size = 784\n",
    "output_size = 10\n",
    "hidden_layer_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e74da816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.keras.Dense(output_size) takes input to the model and finds the dot product of the inputs and weights, and adds the bias.\n",
    "#This is where we apply an activation function to this expression.\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Flatten(input_shape=(28,28,1)),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                            tf.keras.layers.Dense(hidden_layer_size,activation='relu'),\n",
    "                            tf.keras.layers.Dense(output_size,activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3596c16",
   "metadata": {},
   "source": [
    "## Select optimizer and loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "adcbf1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])#If o/ps are one-hot encoded use categorical cross entropy,\n",
    "                                                                        #if not sparse categorical cross entropy\n",
    "\n",
    "                                                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b0e6d460",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "540/540 - 4s - loss: 0.3295 - accuracy: 0.9053 - val_loss: 0.1635 - val_accuracy: 0.9528 - 4s/epoch - 7ms/step\n",
      "Epoch 2/5\n",
      "540/540 - 3s - loss: 0.1350 - accuracy: 0.9597 - val_loss: 0.1187 - val_accuracy: 0.9640 - 3s/epoch - 6ms/step\n",
      "Epoch 3/5\n",
      "540/540 - 3s - loss: 0.0959 - accuracy: 0.9711 - val_loss: 0.0962 - val_accuracy: 0.9708 - 3s/epoch - 5ms/step\n",
      "Epoch 4/5\n",
      "540/540 - 3s - loss: 0.0758 - accuracy: 0.9772 - val_loss: 0.0711 - val_accuracy: 0.9775 - 3s/epoch - 5ms/step\n",
      "Epoch 5/5\n",
      "540/540 - 3s - loss: 0.0585 - accuracy: 0.9815 - val_loss: 0.0602 - val_accuracy: 0.9810 - 3s/epoch - 5ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x17e62273a90>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "model.fit(train_data,epochs=EPOCHS,validation_data=(validation_inputs,validation_targets),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337a61ce",
   "metadata": {},
   "source": [
    "## Using the code from the lecture as the basis, fiddle with the hyperparameters of the algorithm.\n",
    "\n",
    "1. The *width* (the hidden layer size) of the algorithm. Try a hidden layer size of 200. How does the validation accuracy of the model change? What about the time it took the algorithm to train? Can you find a hidden layer size that does better?\n",
    "\n",
    "2. The *depth* of the algorithm. Add another hidden layer to the algorithm. This is an extremely important exercise! How does the validation accuracy change? What about the time it took the algorithm to train? Hint: Be careful with the shapes of the weights and the biases.\n",
    "\n",
    "3. The *width and depth* of the algorithm. Add as many additional layers as you need to reach 5 hidden layers. Moreover, adjust the width of the algorithm as you find suitable. How does the validation accuracy change? What about the time it took the algorithm to train?\n",
    "\n",
    "4. Fiddle with the activation functions. Try applying sigmoid transformation to both layers. The sigmoid activation is given by the string 'sigmoid'.\n",
    "\n",
    "5. Fiddle with the activation functions. Try applying a ReLu to the first hidden layer and tanh to the second one. The tanh activation is given by the string 'tanh'.\n",
    "\n",
    "6. Adjust the batch size. Try a batch size of 10000. How does the required time change? What about the accuracy?\n",
    "\n",
    "7. Adjust the batch size. Try a batch size of 1. That's the SGD. How do the time and accuracy change? Is the result coherent with the theory?\n",
    "\n",
    "8. Adjust the learning rate. Try a value of 0.0001. Does it make a difference?\n",
    "\n",
    "9. Adjust the learning rate. Try a value of 0.02. Does it make a difference?\n",
    "\n",
    "10. Combine all the methods above and try to reach a validation accuracy of 98.5+ percent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea7ebb5",
   "metadata": {},
   "source": [
    "## test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "43fc2282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 299ms/step - loss: 0.0811 - accuracy: 0.9755\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df47bd22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.08. Test accuracy: 97.55%\n"
     ]
    }
   ],
   "source": [
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4868583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
